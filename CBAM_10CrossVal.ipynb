{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Cross Validation with Pretrained(Resnet18) + Attention(CBAM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T06:26:32.011665Z",
     "iopub.status.busy": "2025-01-11T06:26:32.010701Z",
     "iopub.status.idle": "2025-01-11T06:26:38.867636Z",
     "shell.execute_reply": "2025-01-11T06:26:38.866761Z",
     "shell.execute_reply.started": "2025-01-11T06:26:32.011622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from timm import create_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timeit import default_timer as timer\n",
    "# Set device (assuming GPU is available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering 3 Classes for heart US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T06:26:38.870076Z",
     "iopub.status.busy": "2025-01-11T06:26:38.869564Z",
     "iopub.status.idle": "2025-01-11T06:26:41.187013Z",
     "shell.execute_reply": "2025-01-11T06:26:41.186087Z",
     "shell.execute_reply.started": "2025-01-11T06:26:38.870033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Path to the dataset (assuming it's organized into 10 folds with 3 classes)\n",
    "dataset_path = \"\"  # Update as needed\n",
    "\n",
    "# Create a list to store fold data\n",
    "fold_data = []\n",
    "\n",
    "# Class name to label mapping (assuming 3 classes)\n",
    "class_mapping = {\n",
    "    'HCM': 0,  # Label 0 \n",
    "    'HTN': 1,  # Label 1 \n",
    "    'Normal': 2   # Label 2 \n",
    "}\n",
    "\n",
    "# Assuming each fold has 3 subfolders corresponding to the 3 classes\n",
    "for fold in range(1, 11):\n",
    "    fold_images = []\n",
    "    fold_labels = []\n",
    "\n",
    "    # Path for the current fold\n",
    "    fold_path = os.path.join(dataset_path, f\"fold_{fold}\")\n",
    "    if not os.path.isdir(fold_path):\n",
    "        print(f\"Error: {fold_path} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing Fold {fold}:\")\n",
    "\n",
    "    # Iterate through the subdirectories for each class\n",
    "    for class_folder in os.listdir(fold_path):\n",
    "        class_folder_path = os.path.join(fold_path, class_folder)\n",
    "        if not os.path.isdir(class_folder_path):\n",
    "            continue\n",
    "\n",
    "        # Get the label for the class folder using the class_mapping\n",
    "        if class_folder in class_mapping:\n",
    "            label = class_mapping[class_folder]\n",
    "        else:\n",
    "            print(f\"Warning: Class folder '{class_folder}' not in mapping. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Process each image in the class folder\n",
    "        for img_name in os.listdir(class_folder_path):\n",
    "            img_path = os.path.join(class_folder_path, img_name)\n",
    "            fold_images.append(img_path)\n",
    "            fold_labels.append(label)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    fold_images = np.array(fold_images)\n",
    "    fold_labels = np.array(fold_labels)\n",
    "\n",
    "    # Split into 80% train and 20% test+validation\n",
    "    train_images, temp_images, train_labels, temp_labels = train_test_split(\n",
    "        fold_images, fold_labels, test_size=0.2, random_state=seed, stratify=fold_labels\n",
    "    )\n",
    "\n",
    "    # Split the remaining 20% into 10% validation and 10% test\n",
    "    val_images, test_images, val_labels, test_labels = train_test_split(\n",
    "        temp_images, temp_labels, test_size=0.5, random_state=seed, stratify=temp_labels\n",
    "    )\n",
    "\n",
    "    # Print the size of train, validation, and test sets\n",
    "    print(f\"  Train size: {len(train_images)}, Validation size: {len(val_images)}, Test size: {len(test_images)}\")\n",
    "    print(f\"  Class distribution in Train: {np.bincount(train_labels)}\")\n",
    "    print(f\"  Class distribution in Validation: {np.bincount(val_labels)}\")\n",
    "    print(f\"  Class distribution in Test: {np.bincount(test_labels)}\\n\")\n",
    "\n",
    "    # Store fold data\n",
    "    fold_data.append({\n",
    "        \"train_images\": train_images,\n",
    "        \"train_labels\": train_labels,\n",
    "        \"val_images\": val_images,\n",
    "        \"val_labels\": val_labels,\n",
    "        \"test_images\": test_images,\n",
    "        \"test_labels\": test_labels\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T06:27:15.881801Z",
     "iopub.status.busy": "2025-01-11T06:27:15.881489Z",
     "iopub.status.idle": "2025-01-11T06:27:15.909934Z",
     "shell.execute_reply": "2025-01-11T06:27:15.909028Z",
     "shell.execute_reply.started": "2025-01-11T06:27:15.881776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Open image and convert to RGB\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "# Define the image transformations (e.g., normalization, augmentation)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match the input size expected by ResNet\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Channel Attention Module\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
    "        out = self.sigmoid(avg_out + max_out)\n",
    "        return x * out  # Scale the input by the channel attention\n",
    "# Spatial Attention Module\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=(kernel_size // 2), bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # Compress along the channel dimension using max and average pooling\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_compressed = torch.cat([avg_out, max_out], dim=1)  # Combine along channel dimension\n",
    "        attention = self.sigmoid(self.conv(x_compressed))\n",
    "        return x * attention  # Scale the input by the spatial attention\n",
    "# Convolutional Block Attention Module (CBAM)\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "    def forward(self, x):\n",
    "        x = self.channel_attention(x)  # Apply channel attention\n",
    "        x = self.spatial_attention(x)  # Apply spatial attention\n",
    "        return x\n",
    "class ResNet18CBAM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18CBAM, self).__init__()\n",
    "        # Load pre-trained ResNet18\n",
    "        self.resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
    "        # Add CBAM to each residual block\n",
    "        self.resnet18.layer1 = self.add_cbam_to_layer(self.resnet18.layer1)\n",
    "        self.resnet18.layer2 = self.add_cbam_to_layer(self.resnet18.layer2)\n",
    "        self.resnet18.layer3 = self.add_cbam_to_layer(self.resnet18.layer3)\n",
    "        self.resnet18.layer4 = self.add_cbam_to_layer(self.resnet18.layer4)\n",
    "        # Modify the final fully connected layer for num_classes\n",
    "        num_ftrs = self.resnet18.fc.in_features\n",
    "        self.resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    def add_cbam_to_layer(self, layer):\n",
    "        # Add CBAM to each block in the layer\n",
    "        for idx, block in enumerate(layer):\n",
    "            block.add_module(\"cbam\", CBAM(block.conv2.out_channels))\n",
    "        return layer\n",
    "    def forward(self, x):\n",
    "        return self.resnet18(x)\n",
    "def initialize_model():\n",
    "    # Instantiate and move to device\n",
    "    model = ResNet18CBAM(num_classes=3)\n",
    "    model=model.to(device)\n",
    "    return model\n",
    "# Set optimizer and loss function\n",
    "def initialize_optimizer(model):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-4, weight_decay=0.0001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    return optimizer, loss_fn\n",
    "# Train step\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    all_train_preds, all_train_labels = [], []\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        # Handle Inception model's tuple output\n",
    "        if isinstance(outputs, tuple):\n",
    "            y_pred = outputs.logits  # Get the main logits\n",
    "        else:\n",
    "            y_pred = outputs\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Compute accuracy\n",
    "        y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item() / y.size(0)\n",
    "        # Store predictions and labels\n",
    "        all_train_preds.extend(y_pred_class.cpu().numpy())\n",
    "        all_train_labels.extend(y.cpu().numpy())\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    return train_loss, train_acc, all_train_preds, all_train_labels\n",
    "\n",
    "# Validation step\n",
    "def val_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(X)\n",
    "            if isinstance(outputs, tuple):\n",
    "                val_pred_logits = outputs.logits\n",
    "            else:\n",
    "                val_pred_logits = outputs\n",
    "            # Compute loss\n",
    "            loss = loss_fn(val_pred_logits, y)\n",
    "            val_loss += loss.item()\n",
    "            # Compute accuracy\n",
    "            val_pred_labels = torch.argmax(val_pred_logits, dim=1)\n",
    "            val_acc += (val_pred_labels == y).sum().item() / y.size(0)\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(val_pred_labels.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    val_loss /= len(dataloader)\n",
    "    val_acc /= len(dataloader)\n",
    "    return val_loss, val_acc, all_preds, all_labels\n",
    "\n",
    "# Test step\n",
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(X)\n",
    "            if isinstance(outputs, tuple):\n",
    "                test_pred_logits = outputs.logits\n",
    "            else:\n",
    "                test_pred_logits = outputs\n",
    "            # Compute loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            # Compute accuracy\n",
    "            test_pred_labels = torch.argmax(test_pred_logits, dim=1)\n",
    "            test_acc += (test_pred_labels == y).sum().item() / y.size(0)\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(test_pred_labels.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader)\n",
    "    return test_loss, test_acc, all_preds, all_labels\n",
    "# Train loop\n",
    "def train(model, train_dataloader, val_dataloader, test_dataloader, optimizer, loss_fn, epochs, device):\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "    all_train_preds, all_train_labels = [], []\n",
    "    all_val_preds, all_val_labels = [], []\n",
    "    all_test_preds, all_test_labels = [], []\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        # Training step\n",
    "        train_loss, train_acc, train_preds, train_labels = train_step(\n",
    "            model, train_dataloader, loss_fn, optimizer, device\n",
    "        )\n",
    "        # Validation step\n",
    "        val_loss, val_acc, val_preds, val_labels = val_step(\n",
    "            model, val_dataloader, loss_fn, device\n",
    "        )\n",
    "        # Testing step\n",
    "        test_loss, test_acc, test_preds, test_labels = test_step(\n",
    "            model, test_dataloader, loss_fn, device\n",
    "        )\n",
    "        # Store epoch results\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        # Store all predictions and labels\n",
    "        all_train_preds.extend(train_preds)\n",
    "        all_train_labels.extend(train_labels)\n",
    "        all_val_preds.extend(val_preds)\n",
    "        all_val_labels.extend(val_labels)\n",
    "        all_test_preds.extend(test_preds)\n",
    "        all_test_labels.extend(test_labels)\n",
    "        # Log epoch results\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n",
    "            f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    return results, all_train_preds, all_train_labels, all_val_preds, all_val_labels, all_test_preds, all_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T06:27:21.966591Z",
     "iopub.status.busy": "2025-01-11T06:27:21.966218Z",
     "iopub.status.idle": "2025-01-11T08:26:43.322199Z",
     "shell.execute_reply": "2025-01-11T08:26:43.321215Z",
     "shell.execute_reply.started": "2025-01-11T06:27:21.966553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(cm):\n",
    "    num_classes = cm.shape[0]\n",
    "    \n",
    "    # Initialize empty lists to store metrics for each class\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    specificity = []\n",
    "    \n",
    "    # Initialize counters for total TP, FP, TN, FN for overall accuracy calculation\n",
    "    TP_total = 0\n",
    "    TN_total = 0\n",
    "    FP_total = 0\n",
    "    FN_total = 0\n",
    "    total_samples = cm.sum()  \n",
    "    \n",
    "    # Iterate through each class\n",
    "    for i in range(num_classes):\n",
    "        TP = cm[i, i]  # True Positive for class i\n",
    "        FP = cm[:, i].sum() - TP  # False Positive for class i\n",
    "        FN = cm[i, :].sum() - TP  # False Negative for class i\n",
    "        \n",
    "        # Calculate True Negative correctly by summing all non-class predictions\n",
    "        TN = total_samples - (TP + FP + FN) \n",
    "\n",
    "        # Accumulate TP, FP, TN, FN for overall accuracy\n",
    "        TP_total += TP\n",
    "        TN_total += TN\n",
    "        FP_total += FP\n",
    "        FN_total += FN\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision_i = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        precision.append(precision_i)\n",
    "        \n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall_i = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        recall.append(recall_i)\n",
    "        \n",
    "        # F1-Score: Harmonic mean of precision and recall\n",
    "        if precision_i + recall_i > 0:\n",
    "            f1_i = 2 * (precision_i * recall_i) / (precision_i + recall_i)\n",
    "        else:\n",
    "            f1_i = 0\n",
    "        f1_score.append(f1_i)\n",
    "        \n",
    "        # Specificity: TN / (TN + FP)\n",
    "        specificity_i = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        specificity.append(specificity_i)\n",
    "\n",
    "    # Correct Accuracy calculation using the new formula\n",
    "    accuracy = (TP_total + TN_total) / (TP_total + TN_total + FP_total + FN_total) if total_samples > 0 else np.nan\n",
    "\n",
    "    precision_macro = np.mean(precision)\n",
    "    recall_macro = np.mean(recall)\n",
    "    f1_macro = np.mean(f1_score)\n",
    "    specificity_macro = np.mean(specificity)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'specificity_macro': specificity_macro\n",
    "    }\n",
    "# Train, validation, and test loop with manual metric calculation\n",
    "for fold_index, fold in enumerate(fold_data):\n",
    "    print(f\"\\nProcessing Fold {fold_index + 1}...\")\n",
    "    \n",
    "    # Get the data for the current fold\n",
    "    train_images = fold[\"train_images\"]\n",
    "    train_labels = fold[\"train_labels\"]\n",
    "    val_images = fold[\"val_images\"]  # Added validation images\n",
    "    val_labels = fold[\"val_labels\"]  # Added validation labels\n",
    "    test_images = fold[\"test_images\"]\n",
    "    test_labels = fold[\"test_labels\"]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset(train_images, train_labels, transform=transform)\n",
    "    val_dataset = CustomDataset(val_images, val_labels, transform=transform)  # Validation dataset\n",
    "    test_dataset = CustomDataset(test_images, test_labels, transform=transform)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)  # Validation dataloader\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = initialize_model()\n",
    "    optimizer, loss_fn = initialize_optimizer(model)\n",
    "\n",
    "    # Start training\n",
    "    start_time = timer()\n",
    "    results, train_preds, train_labels, val_preds, val_labels, test_preds, test_labels = train(\n",
    "        model, train_dataloader, val_dataloader, test_dataloader, optimizer, loss_fn, epochs=10, device=device)\n",
    "    end_time = timer()\n",
    "\n",
    "    # Flatten labels and predictions to 1D arrays\n",
    "    train_labels = np.array(train_labels).flatten()\n",
    "    train_preds = np.array(train_preds).flatten()\n",
    "    val_labels = np.array(val_labels).flatten()  # Validation labels\n",
    "    val_preds = np.array(val_preds).flatten()    # Validation predictions\n",
    "    test_labels = np.array(test_labels).flatten()\n",
    "    test_preds = np.array(test_preds).flatten()\n",
    "    \n",
    "    # Confusion Matrices\n",
    "    train_cm = confusion_matrix(train_labels, train_preds)\n",
    "    val_cm = confusion_matrix(val_labels, val_preds)  # Validation confusion matrix\n",
    "    test_cm = confusion_matrix(test_labels, test_preds)\n",
    "    \n",
    "    print(f\"\\nFold {fold_index + 1} Confusion Matrices:\")\n",
    "    print(\"\\nTrain Confusion Matrix:\")\n",
    "    print(train_cm)\n",
    "    print(\"\\nValidation Confusion Matrix:\")\n",
    "    print(val_cm)\n",
    "    print(\"\\nTest Confusion Matrix:\")\n",
    "    print(test_cm)\n",
    "    \n",
    "    # Calculate Train Metrics manually\n",
    "    train_metrics = calculate_metrics(train_cm)\n",
    "    print(\"\\nTrain Metrics:\")\n",
    "    print(f\"Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision (Macro Avg): {train_metrics['precision_macro']:.4f}\")\n",
    "    print(f\"Recall/Sensitivity (Macro Avg): {train_metrics['recall_macro']:.4f}\")\n",
    "    print(f\"F1-Score (Macro Avg): {train_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Specificity (Macro Avg): {train_metrics['specificity_macro']:.4f}\")\n",
    "\n",
    "    # Calculate Validation Metrics manually\n",
    "    val_metrics = calculate_metrics(val_cm)\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision (Macro Avg): {val_metrics['precision_macro']:.4f}\")\n",
    "    print(f\"Recall/Sensitivity (Macro Avg): {val_metrics['recall_macro']:.4f}\")\n",
    "    print(f\"F1-Score (Macro Avg): {val_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Specificity (Macro Avg): {val_metrics['specificity_macro']:.4f}\")\n",
    "\n",
    "    # Calculate Test Metrics manually\n",
    "    test_metrics = calculate_metrics(test_cm)\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision (Macro Avg): {test_metrics['precision_macro']:.4f}\")\n",
    "    print(f\"Recall/Sensitivity (Macro Avg): {test_metrics['recall_macro']:.4f}\")\n",
    "    print(f\"F1-Score (Macro Avg): {test_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Specificity (Macro Avg): {test_metrics['specificity_macro']:.4f}\")\n",
    "\n",
    "    # Print training time for each fold\n",
    "    print(f\"Training time for fold {fold_index + 1}: {end_time - start_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6211530,
     "sourceId": 10076682,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6254568,
     "sourceId": 10134403,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6364791,
     "sourceId": 10285178,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
